# ğŸ§ Deepfake Audio Detection with Explainable AI (XAI)

This project implements an **audio-only deepfake detection system** using deep learning and **multiple Explainable AI (XAI) techniques**.
Users can upload a `.wav` file, run inference using different models, and visually compare **Grad-CAM, LIME, and SHAP explanations** side-by-side in a Streamlit web application.

---

## ğŸš€ Running the Streamlit Application

### 1ï¸âƒ£ Create and activate a virtual environment

From the project root:

```bash
python -m venv venv
```

**Windows**

```bash
venv\Scripts\activate
```

**macOS / Linux**

```bash
source venv/bin/activate
```

---

### 2ï¸âƒ£ Install dependencies

Install all required packages using:

```bash
pip install -r requirements.txt
```

> âš ï¸ This project was tested with **TensorFlow 2.12.0**.
> Using other versions may lead to incompatibilities.

---

### 3ï¸âƒ£ Run the Streamlit app

```bash
streamlit run app/app.py
```

The application will open automatically in your browser.

---

## ğŸµ Audio Dataset for Inference (Using the App)

To **use the app for inference only**, you may upload **any `.wav` file** directly through the Streamlit interface.

### ğŸ“Œ Recommended dataset (optional, for testing)

You can use the **Fake-or-Real Audio Dataset** from Kaggle, specifically the **`for-2sec` version**:

ğŸ”— **Dataset link:**
[https://www.kaggle.com/datasets/mohammedabdeldayem/the-fake-or-real-dataset](https://www.kaggle.com/datasets/mohammedabdeldayem/the-fake-or-real-dataset)

For inference and demo purposes, we recommend using:

```bash
for-2sec/for-2seconds/testing/
â”œâ”€â”€ real/
â””â”€â”€ fake/
```

This subset contains short, standardized 2-second audio samples and is suitable for:

* Manual testing
* Demo purposes
* Model evaluation

You **do NOT** need to place inference audio files inside the project folders â€”
they can be uploaded directly through the Streamlit interface.

---

## ğŸ” Retraining the Models (Optional)

If you want to **retrain the audio classification models**, follow the steps below.

---

### 1ï¸âƒ£ Download the training dataset

Download the **Fake-or-Real Audio Dataset** from Kaggle and use the **`for-2sec` version**:

ğŸ”— [https://www.kaggle.com/datasets/mohammedabdeldayem/the-fake-or-real-dataset](https://www.kaggle.com/datasets/mohammedabdeldayem/the-fake-or-real-dataset)

You will work with:

```bash
for-2sec/for-2seconds/
â”œâ”€â”€ training/
â”œâ”€â”€ validation/
â””â”€â”€ testing/
```

---

### 2ï¸âƒ£ Place raw audio files (IMPORTANT)

Before converting to spectrograms, place the raw `.wav` files into the following structure:

```bash
data/
â””â”€â”€ audio/
    â”œâ”€â”€ train/
    â”‚   â”œâ”€â”€ real/
    â”‚   â””â”€â”€ fake/
    â”œâ”€â”€ val/
    â”‚   â”œâ”€â”€ real/
    â”‚   â””â”€â”€ fake/
    â””â”€â”€ test/
        â”œâ”€â”€ real/
        â””â”€â”€ fake/
```

These folders should be populated using the corresponding splits from:

* `for-2seconds/training`
* `for-2seconds/validation`
* `for-2seconds/testing`

Folders must be renamed to `train`, `val`, and `test`

---

### 3ï¸âƒ£ Convert WAV files to spectrograms

Use the provided script:

```bash
scripts/wav_to_spectrogram.py
```

This script:

* Reads `.wav` files from `data/audio/`
* Converts them into **Mel-spectrogram images**
* Automatically creates and populates:

```bash
data/
â””â”€â”€ spectrograms/
    â”œâ”€â”€ train/
    â”‚   â”œâ”€â”€ real/
    â”‚   â””â”€â”€ fake/
    â”œâ”€â”€ val/
    â”‚   â”œâ”€â”€ real/
    â”‚   â””â”€â”€ fake/
    â””â”€â”€ test/
        â”œâ”€â”€ real/
        â””â”€â”€ fake/
```

**Important notes:**

* Folder names must be exactly `train`, `val`, and `test`
* Images are saved as `.png`
* All spectrograms are resized to **224Ã—224**
* This format is required for all models

---

### 4ï¸âƒ£ Train the audio models

From the project root, run the following scripts (order does not matter):

#### â–¶ MobileNet

```bash
python training/train_audio_mobilenet.py
```

#### â–¶ VGG16

```bash
python training/train_audio_vgg.py
```

#### â–¶ ResNet50

```bash
python training/train_audio_resnet.py
```

#### â–¶ Custom CNN

```bash
python training/train_audio_custom_cnn.py
```

Each script will:

* Load spectrogram images from `data/spectrograms/`
* Train the model
* Save it automatically to:

```bash
weights/audio/
```

Example:

```bash
weights/audio/mobilenet/
weights/audio/vgg16/
weights/audio/resnet50/
weights/audio/custom_cnn/
```

---

### 5ï¸âƒ£ Use retrained models in the app

No code changes are required.

The Streamlit app automatically loads models from:

```bash
weights/audio/
```

As long as the folder names remain unchanged, newly trained models will be used automatically.

---

## ğŸ§  Explainability Methods Included

The application provides **side-by-side explainability comparison** using:

* **Grad-CAM** â€” highlights discriminative spectrogram regions
* **LIME** â€” local, superpixel-based explanations
* **SHAP** â€” contribution-based explanations with neutral (gray) regions

All XAI methods are applied automatically to the selected model.

---

## ğŸ“ Project Structure (Current Scope)

```bash
unified-xai/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ audio_utils.py
â”‚   â”œâ”€â”€ gradcam_utils.py
â”‚   â”œâ”€â”€ lime_utils.py
â”‚   â””â”€â”€ shap_utils.py
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ train_audio_mobilenet.py
â”‚   â”œâ”€â”€ train_audio_vgg.py
â”‚   â”œâ”€â”€ train_audio_resnet.py
â”‚   â””â”€â”€ train_audio_custom_cnn.py
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ wav_to_spectrogram.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ audio/
â”‚   â””â”€â”€ spectrograms/
â”œâ”€â”€ weights/
â”‚   â””â”€â”€ audio/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âœ… Current Project Scope

âœ” Audio-based deepfake detection
âœ” Multiple CNN architectures
âœ” Explainable AI (Grad-CAM, LIME, SHAP)
âœ” Unified Streamlit interface
âœ” Side-by-side XAI comparison