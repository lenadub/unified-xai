# ğŸ§ Deepfake Audio Detection with Explainable AI (XAI)

This project implements an **audio-only deepfake detection system** using deep learning and **multiple Explainable AI (XAI) techniques**.
Users can upload a `.wav` file, run inference using different models, and visually compare **Grad-CAM, LIME, and SHAP explanations** side-by-side in a Streamlit web application.

---

## ğŸš€ Running the Streamlit Application

### 1ï¸âƒ£ Create and activate a virtual environment

From the project root:

```bash
python -m venv venv
```

**Windows**

```bash
venv\Scripts\activate
```

**macOS / Linux**

```bash
source venv/bin/activate
```

---

### 2ï¸âƒ£ Install dependencies

Install all required packages using:

```bash
pip install -r requirements.txt
```

> âš ï¸ This project was tested with **TensorFlow 2.12.0**.
> Using other versions may lead to incompatibilities.

---

### 3ï¸âƒ£ Run the Streamlit app

```bash
streamlit run app/app.py
```

The application will open automatically in your browser.

---

## ğŸµ Audio Dataset for Inference (Using the App)

To **use the app for inference only**, you may upload **any `.wav` file** directly through the Streamlit interface.

### ğŸ“Œ Recommended dataset (optional, for testing)

You can use the **Fake-or-Real Audio Dataset** from Kaggle, specifically the **`for-2sec` version**:

ğŸ”— **Dataset link:**
[https://www.kaggle.com/datasets/mohammedabdeldayem/the-fake-or-real-dataset](https://www.kaggle.com/datasets/mohammedabdeldayem/the-fake-or-real-dataset)

For inference and demo purposes, we recommend using:

```bash
for-2sec/for-2seconds/testing/
â”œâ”€â”€ real/
â””â”€â”€ fake/
```

This subset contains short, standardized 2-second audio samples and is suitable for:

* Manual testing
* Demo purposes
* Model evaluation

You **do NOT** need to place inference audio files inside the project folders â€”
they can be uploaded directly through the Streamlit interface.

---

## ğŸ” Retraining the Models (Optional)

If you want to **retrain the audio classification models**, follow the steps below.

---

### 1ï¸âƒ£ Download the training dataset

Download the **Fake-or-Real Audio Dataset** from Kaggle and use the **`for-2sec` version**:

ğŸ”— [https://www.kaggle.com/datasets/mohammedabdeldayem/the-fake-or-real-dataset](https://www.kaggle.com/datasets/mohammedabdeldayem/the-fake-or-real-dataset)

You will work with:

```bash
for-2sec/for-2seconds/
â”œâ”€â”€ training/
â”œâ”€â”€ validation/
â””â”€â”€ testing/
```

---

### 2ï¸âƒ£ Place raw audio files (IMPORTANT)

Before converting to spectrograms, place the raw `.wav` files into the following structure:

```bash
data/
â””â”€â”€ audio/
    â”œâ”€â”€ train/
    â”‚   â”œâ”€â”€ real/
    â”‚   â””â”€â”€ fake/
    â”œâ”€â”€ val/
    â”‚   â”œâ”€â”€ real/
    â”‚   â””â”€â”€ fake/
    â””â”€â”€ test/
        â”œâ”€â”€ real/
        â””â”€â”€ fake/
```

These folders should be populated using the corresponding splits from:

* `for-2seconds/training`
* `for-2seconds/validation`
* `for-2seconds/testing`

Folders must be renamed to `train`, `val`, and `test`

---

### 3ï¸âƒ£ Convert WAV files to spectrograms

Use the provided script:

```bash
scripts/wav_to_spectrogram.py
```

This script:

* Reads `.wav` files from `data/audio/`
* Converts them into **Mel-spectrogram images**
* Automatically creates and populates:

```bash
data/
â””â”€â”€ spectrograms/
    â”œâ”€â”€ train/
    â”‚   â”œâ”€â”€ real/
    â”‚   â””â”€â”€ fake/
    â”œâ”€â”€ val/
    â”‚   â”œâ”€â”€ real/
    â”‚   â””â”€â”€ fake/
    â””â”€â”€ test/
        â”œâ”€â”€ real/
        â””â”€â”€ fake/
```

**Important notes:**

* Folder names must be exactly `train`, `val`, and `test`
* Images are saved as `.png`
* All spectrograms are resized to **224Ã—224**
* This format is required for all models

---

### 4ï¸âƒ£ Train the audio models

From the project root, run the following scripts (order does not matter):

#### â–¶ MobileNet

```bash
python training/train_audio_mobilenet.py
```

#### â–¶ VGG16

```bash
python training/train_audio_vgg.py
```

#### â–¶ ResNet50

```bash
python training/train_audio_resnet.py
```

#### â–¶ Custom CNN

```bash
python training/train_audio_custom_cnn.py
```

Each script will:

* Load spectrogram images from `data/spectrograms/`
* Train the model
* Save it automatically to:

```bash
weights/audio/
```

Example:

```bash
weights/audio/mobilenet/
weights/audio/vgg16/
weights/audio/resnet50/
weights/audio/custom_cnn/
```

---

### 5ï¸âƒ£ Use retrained models in the app

No code changes are required.

The Streamlit app automatically loads models from:

```bash
weights/audio/
```

As long as the folder names remain unchanged, newly trained models will be used automatically.

---

## ğŸ§  Explainability Methods Included

The application provides **side-by-side explainability comparison** using:

* **Grad-CAM** â€” highlights discriminative spectrogram regions
* **LIME** â€” local, superpixel-based explanations
* **SHAP** â€” contribution-based explanations with neutral (gray) regions

All XAI methods are applied automatically to the selected model.

---

## Chest X-Ray Image Detection with Explainable AI (XAI)

In addition to audio deepfake detection, this unified project also includes an image-based chest X-ray classification pipeline (Normal vs. Malignant), paired with Explainable AI (XAI) methods to interpret model decisions.

Users can upload a chest X-ray image (.png, .jpg, .jpeg), run inference with a supported model, and visualize explanations such as Grad-CAM, LIME, and SHAP (when compatible).

## Image Dataset (CheXpert)

The image pipeline is based on the CheXpert chest radiograph dataset from Stanford ML Group.

Dataset reference:
https://stanfordmlgroup.github.io/competitions/chexpert/

For this project, the task is simplified to a binary classification setup:

- Normal
- Malignant / Lung Cancer
- Note: You may use any chest X-ray dataset for demo/inference purposes, as long as inputs follow the expected format.

## Image Data Organization

If you want to run training or evaluation, place images using the following structure:

```bash
data/
â””â”€â”€ images/
    â”œâ”€â”€ train/
    â”‚   â”œâ”€â”€ normal/
    â”‚   â””â”€â”€ cancer/
    â”œâ”€â”€ val/
    â”‚   â”œâ”€â”€ normal/
    â”‚   â””â”€â”€ cancer/
    â””â”€â”€ test/
        â”œâ”€â”€ normal/
        â””â”€â”€ cancer/
```

Requirements:

- Supported formats: .png, .jpg, .jpeg
- Images are resized to 224Ã—224 during preprocessing
- Normalization is handled automatically by the pipeline

## Training the Image Models (Optional)

If you want to retrain the image classification models, run the training scripts (PyTorch):

AlexNet
```bash
python training/train_image_alexnet.py
```
DenseNet121
```bash
python training/train_image_densenet.py
```

Each script will:

- load images from data/images/
- train the model
- save weights to:
```bash
weights/image/
â”œâ”€â”€ alexnet/
â””â”€â”€ densenet121/
```

Models are stored as PyTorch checkpoints (.pth).

## Explainability Methods for Images

The image pipeline supports the following explainability techniques:

- Grad-CAM â€” highlights the most discriminative regions in the X-ray
- LIME â€” local explanations using superpixel perturbations
- SHAP â€” contribution-based explanations (availability may depend on model/input constraints)

All methods are automatically filtered based on the input type and selected model.

## Unified Multi-Modal XAI Interface (Audio + Image)

This project refactors and merges two repositories into a single Streamlit interface that supports:

- Audio deepfake detection (.wav)
- Chest X-ray classification (.png, .jpg, .jpeg)
- Multiple deep learning models per modality
- Multiple XAI methods with automatic compatibility filtering
- A dedicated comparison tab for side-by-side analysis

## Streamlit Tabs Overview
### Tab 1 â€” Prediction

The Prediction tab is designed for fast interaction and demo:
- Select input type (Audio or Image)
- Upload a file
- Select a compatible model
- Select one or multiple XAI methods
- Run inference and display:
 - predicted label + confidence
 - selected XAI visualizations

This tab ensures basic functionality (as required in the instructions):
1 dataset + 1 model + 1 XAI method â†’ explanation output.

### Tab 2 â€” XAI Comparison (All Compatible Methods)

The XAI Comparison tab implements the project requirement for systematic comparison:

Even if the user selects only one method in Tab 1, Tab 2 computes and displays all XAI methods compatible with the last prediction (input type + selected model).

Workflow:
- Tab 1 stores the latest prediction context (model, preprocessed input x, predicted class index)
- Tab 2 retrieves this context and recomputes all compatible XAI methods
- Results are displayed side-by-side (or in a stacked grid) with clear method labels

This ensures a fair and consistent comparison of explainability techniques for the exact same input and model.

## Automatic XAI Compatibility Filtering

XAI options are filtered automatically so that only applicable methods appear for a given modality/model.

Example:
- If the user uploads an audio file, image-only XAI methods are hidden/disabled
- If the user uploads an image, audio-specific logic is not exposed

This avoids invalid selections and improves usability.

## ğŸ“ Project Structure (Current Scope)

```bash
unified-xai/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ audio/
â”‚       â”œâ”€â”€ audio_pipeline.py
â”‚       â”œâ”€â”€ audio_utils.py
â”‚       â””â”€â”€ audio_xai.py
â”‚   â”œâ”€â”€ image/
â”‚       â”œâ”€â”€ image_pipeline.py
â”‚       â”œâ”€â”€ gradcam.py.py
â”‚       â””â”€â”€ image_xai.py
â”‚   â”œâ”€â”€ gradcam_utils.py
â”‚   â”œâ”€â”€ lime_utils.py
â”‚   â””â”€â”€ shap_utils.py
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ train_audio_mobilenet.py
â”‚   â”œâ”€â”€ train_audio_vgg.py
â”‚   â”œâ”€â”€ train_audio_resnet.py
â”‚   â”œâ”€â”€ train_audio_custom_cnn.py
â”‚   â”œâ”€â”€ train_image_alexnet.py
â”‚   â”œâ”€â”€ train_image_densenet.py
â”‚   â”œâ”€â”€ evaluate_image_model_alexnet.py
â”‚   â””â”€â”€ evaluate_image_modeldensenet.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ wav_to_spectrogram.py
â”‚   â”œâ”€â”€ demo_gradcam_densenet.py
â”‚   â””â”€â”€ prepare_chexpert.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ audio/
â”‚   â””â”€â”€ spectrograms/
â”œâ”€â”€ weights/
â”‚   â””â”€â”€ audio/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âœ… Current Project Scope

âœ” Audio-based deepfake detection
âœ” Multiple CNN architectures
âœ” Explainable AI (Grad-CAM, LIME, SHAP)
âœ” Unified Streamlit interface
âœ” Side-by-side XAI comparison